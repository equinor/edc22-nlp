{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning & fine tuning\n",
    "\n",
    "So far we know about using pre-trained huggingface language models (e.g BERT, T5), in this part of the workshop we are going to talk about fine-tuning these pre-trained models for specific downstream NLP tasks (e.g. document classification (sentiment), or summarisation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is generally know as transfer learning. Transfer learning is a machine learning technique for adapting pretrained models to solve specialized problems. Sequential transfer learning is learning on one task or one dataset and then transferring this learning to another task or dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:fg:1: no job control in this shell.\n"
     ]
    }
   ],
   "source": [
    "!%pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset: The Yelp Review Full dataset for text classification.\n",
    "Before we can fine-tune a pretrained model, we need to download a dataset and prepare it for training. We are going to use the Yelp dataset for fine-tuning. \n",
    "\n",
    "This dataset is a subset of businesses, reviews and user data.\n",
    "\n",
    "The dataset contains text and the corresponding label (1-5 stars).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset yelp_review_full (/Users/JENSAM/GIT/edc22-nlp/data/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 310.94it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 650000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"/Users/JENSAM/GIT/edc22-nlp/data/yelp_review_full.py\", cache_dir='/Users/JENSAM/GIT/edc22-nlp/data') \n",
    "dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'text': \"Eat at your own risk. The service is terrible, the staff seem to be generally clueless, the management is inclined to blame the staff for their own mistakes, and there's no sense of FAST in their fast food. When we came, half of the menu board was still on breakfast, and it was 4:30p. The only thing they have going for them is that the food is hot and tastes just like McDonald's should. \\\\n\\\\nThen again, the franchise is owned by Rice, and I've come to take terrible service is their MO.\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember we need to process the text using a tokenizer, we will use padding and truncation to handle any variations in the sequence lengths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 650/650 [01:25<00:00,  7.60ba/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:06<00:00,  7.66ba/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the time it takes for training we can create smaller subsets of the full dataset for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /Users/JENSAM/GIT/edc22-nlp/data/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf/cache-a0e621c27d9b360e.arrow\n",
      "Loading cached shuffled indices for dataset at /Users/JENSAM/GIT/edc22-nlp/data/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf/cache-61e0da4d9cd46a2c.arrow\n"
     ]
    }
   ],
   "source": [
    "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = dataset[\"test\"].shuffle(seed=42).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 3,\n",
       " 'text': \"Went here with my roommate last week for a late breakfast and was pleasantly surprised! I got the Eggs Benedict and a chocolate croissant while my roommate went for the French toast with fresh fruit. Everything was very tasty, although my roommate could have easily handled a larger portion. \\\\nAll the employees were super sweet and the French owner's eclectic playlist really added to how much I enjoyed the experience. Will have to go again for lunch or dinner!\"}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_train_dataset[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'text'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "We will be using Hugging Face Transformers Trainer class for training. The API supports a wide range of training options & features.\n",
    "\n",
    "First we need to load the model we are going to fine-tune for a classifcation task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model_name = \"bert-base-cased\" \n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about what this warning is telling us ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to specify where to save the training checkpoints using the TrainingArguments class, this class contains all the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate our models performance we need to pass the Trainer a function for computing and reporting the metrics, you can load different metrics with the load_metric function (e.g. accuracy, precision, recall, f1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is a call to the compute method on metric to calculate the prediction accuracies. Predictions must first be converted to logits, which are the raw predictions of the last layer of the neural network.\n",
    "\n",
    "We use the Argmax and SoftMax functions to make the output values from the neural network be between 0 and 1.\n",
    "The Argmax function interprets the largest positive output value as 1 and all other values as 0. This gives us the predicted class.\n",
    "The SoftMax function gives us the probabilities for the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To monitor the evaluation metrics during fine-tuning we need to specify an evaluation_strategy parameter in the training arguments, in this case at the end of an epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", num_train_epochs=3, evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: sentencepiece in ./env/lib/python3.10/site-packages (0.1.96)\n",
      "Collecting absl-py\n",
      "  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m123.4/123.4 KB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in ./env/lib/python3.10/site-packages (from rouge-score) (1.23.1)\n",
      "Requirement already satisfied: six>=1.14.0 in ./env/lib/python3.10/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./env/lib/python3.10/site-packages (from nltk) (2022.7.24)\n",
      "Requirement already satisfied: tqdm in ./env/lib/python3.10/site-packages (from nltk) (4.64.0)\n",
      "Collecting click\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.6/96.6 KB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in ./env/lib/python3.10/site-packages (from nltk) (1.1.0)\n",
      "Using legacy 'setup.py install' for rouge-score, since package 'wheel' is not installed.\n",
      "Installing collected packages: click, absl-py, nltk, rouge-score\n",
      "  Running setup.py install for rouge-score ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed absl-py-1.2.0 click-8.1.3 nltk-3.7 rouge-score-0.1.2\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2 is available.\n",
      "You should consider upgrading via the '/Users/JENSAM/GIT/edc22-nlp/env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/JENSAM/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!%pip install rouge-score nltk sentencepiece\n",
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Running `brew update --auto-update`...\n",
      "\u001b[34m==>\u001b[0m \u001b[1mAuto-updated Homebrew!\u001b[0m\n",
      "Updated 3 taps (homebrew/core, homebrew/cask and homebrew/cask-fonts).\n",
      "\u001b[34m==>\u001b[0m \u001b[1mNew Formulae\u001b[0m\n",
      "cargo-depgraph             dura                       prql-compiler\n",
      "chain-bench                page                       tlsx\n",
      "\u001b[34m==>\u001b[0m \u001b[1mNew Casks\u001b[0m\n",
      "polypad             qwerty-fr           shop-different      workman\n",
      "\n",
      "You have \u001b[1m6\u001b[0m outdated formulae installed.\n",
      "You can upgrade them with \u001b[1mbrew upgrade\u001b[0m\n",
      "or list them with \u001b[1mbrew outdated\u001b[0m.\n",
      "\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/git-lfs/manifests/3.2.0\u001b[0m\n",
      "######################################################################## 100.0%\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/git-lfs/blobs/sha256:a81d5b29d1\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading from https://pkg-containers.githubusercontent.com/ghcr1/blobs/sh\u001b[0m\n",
      "######################################################################## 100.0%\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring git-lfs--3.2.0.arm64_monterey.bottle.tar.gz\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mCaveats\u001b[0m\n",
      "Update your git config to finish installation:\n",
      "\n",
      "  # Update global git config\n",
      "  $ git lfs install\n",
      "\n",
      "  # Update system git config\n",
      "  $ git lfs install --system\n",
      "\u001b[34m==>\u001b[0m \u001b[1mSummary\u001b[0m\n",
      "ðŸº  /opt/homebrew/Cellar/git-lfs/3.2.0: 3 files, 11.0MB\n",
      "\u001b[34m==>\u001b[0m \u001b[1mRunning `brew cleanup git-lfs`...\u001b[0m\n",
      "Disable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.\n",
      "Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n"
     ]
    }
   ],
   "source": [
    "!apt install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"t5-small\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset xsum/default (download: 245.38 MiB, generated: 507.60 MiB, post-processed: Unknown size, total: 752.98 MiB) to /Users/JENSAM/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255M/255M [01:04<00:00, 3.98MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Downloading data: 2.72MB [00:00, 19.9MB/s]\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:06<00:00, 33.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset xsum downloaded and prepared to /Users/JENSAM/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 149.83it/s]\n",
      "Downloading builder script: 5.60kB [00:00, 5.32MB/s]                   \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "raw_datsets = load_dataset(\"xsum\")\n",
    "metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 204045\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 11332\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 11334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 'The full cost of damage in Newton Stewart, one of the areas worst affected, is still being assessed.\\nRepair work is ongoing in Hawick and many roads in Peeblesshire remain badly affected by standing water.\\nTrains on the west coast mainline face disruption due to damage at the Lamington Viaduct.\\nMany businesses and householders were affected by flooding in Newton Stewart after the River Cree overflowed into the town.\\nFirst Minister Nicola Sturgeon visited the area to inspect the damage.\\nThe waters breached a retaining wall, flooding many commercial properties on Victoria Street - the main shopping thoroughfare.\\nJeanette Tate, who owns the Cinnamon Cafe which was badly affected, said she could not fault the multi-agency response once the flood hit.\\nHowever, she said more preventative work could have been carried out to ensure the retaining wall did not fail.\\n\"It is difficult but I do think there is so much publicity for Dumfries and the Nith - and I totally appreciate that - but it is almost like we\\'re neglected or forgotten,\" she said.\\n\"That may not be true but it is perhaps my perspective over the last few days.\\n\"Why were you not ready to help us a bit more when the warning and the alarm alerts had gone out?\"\\nMeanwhile, a flood alert remains in place across the Borders because of the constant rain.\\nPeebles was badly hit by problems, sparking calls to introduce more defences in the area.\\nScottish Borders Council has put a list on its website of the roads worst affected and drivers have been urged not to ignore closure signs.\\nThe Labour Party\\'s deputy Scottish leader Alex Rowley was in Hawick on Monday to see the situation first hand.\\nHe said it was important to get the flood protection plan right but backed calls to speed up the process.\\n\"I was quite taken aback by the amount of damage that has been done,\" he said.\\n\"Obviously it is heart-breaking for people who have been forced out of their homes and the impact on businesses.\"\\nHe said it was important that \"immediate steps\" were taken to protect the areas most vulnerable and a clear timetable put in place for flood prevention plans.\\nHave you been affected by flooding in Dumfries and Galloway or the Borders? Tell us about your experience of the situation and how it was handled. Email us on selkirk.news@bbc.co.uk or dumfries@bbc.co.uk.',\n",
       " 'summary': 'Clean-up operations are continuing across the Scottish Borders and Dumfries and Galloway after flooding caused by Storm Frank.',\n",
       " 'id': '35232142'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datsets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function show_random_elements can be used to show some randomly picked examples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickletools\n",
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "        print(dataset[picks])\n",
    "   \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    #display(HTML(df.to_html()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'document': ['Officers turned to Twitter in a bid to find a thief who stole Â£600 worth of cosmetics from a local Boots store.\\n\"We are looking for a 40-year-old man who looks 20, glowing skin, long eyelashes, raised eyebrows & pronounced lips,\" they added.\\nIn response, one pun-loving joker replied: \"Is there any foundation to these allegations?\"\\nEnd of Twitter post  by @MonklandsPol\\nThe post by Monklands police sparked a series of witty responses from their followers on the social media platform.\\nReferring to a popular brand of make-up, one asked: \"If you put him in an identity parade, will he be No 7 in the line up?\"\\nAnother said: \"When questioned as to why he had allegedly stolen Â£600 of cosmetics the suspect simply answered \\'Because I\\'m worth it.\\'\"\\nThe theft happened at Boots in Main Street, Coatbridge, at about 12:30 on Thursday.\\nAnyone with information is asked to call Police Scotland on 101 or Crimestoppers.'], 'summary': ['A light-hearted appeal to trace a shoplifter by police in Lanarkshire has become a social media hit.'], 'id': ['40898320']}\n",
      "{'document': ['Officers turned to Twitter in a bid to find a thief who stole Â£600 worth of cosmetics from a local Boots store.\\n\"We are looking for a 40-year-old man who looks 20, glowing skin, long eyelashes, raised eyebrows & pronounced lips,\" they added.\\nIn response, one pun-loving joker replied: \"Is there any foundation to these allegations?\"\\nEnd of Twitter post  by @MonklandsPol\\nThe post by Monklands police sparked a series of witty responses from their followers on the social media platform.\\nReferring to a popular brand of make-up, one asked: \"If you put him in an identity parade, will he be No 7 in the line up?\"\\nAnother said: \"When questioned as to why he had allegedly stolen Â£600 of cosmetics the suspect simply answered \\'Because I\\'m worth it.\\'\"\\nThe theft happened at Boots in Main Street, Coatbridge, at about 12:30 on Thursday.\\nAnyone with information is asked to call Police Scotland on 101 or Crimestoppers.', 'The claim: The European Union would lose out more than the UK from the introduction of tariffs.\\nReality Check verdict: The EU would risk more in cash terms. The UK would risk a higher proportion of its exports.\\n\"Don\\'t just look at it from a UK perspective,\" he said.\\n\"The European Union has a massive surplus in goods with the UK. Who does it harm more if we end up in a new tariff environment?\"\\nThe EU does indeed have a trade surplus with the UK in goods, which means that EU countries sell more goods to the UK than they buy from us.\\nThe UK economy, which is dominated by the service sector, has a trading surplus in services with the EU, which is not big enough to cancel out the deficit in goods.\\nBut Dr Fox was talking about goods, not services.\\nIn 2015, the UK trade figures show the UK exported Ã‚Â£134bn worth of goods to the EU and imported Ã‚Â£223bn worth.\\nThe question Dr Fox asked was who it would harm more if there were tariffs introduced, which damaged that trade.\\nIn cash terms, clearly the EU has more to lose. But in percentage terms the picture is different.\\nUK exports of goods to the EU in 2015 accounted for 47% of total goods exports. According to the NIESR, EU goods exports to the UK account for about 16% of its total exports of goods.\\nSo if you\\'re looking at whether the UK or the EU would risk a greater proportion of their trade from tariffs, clearly the UK would lose more.\\nThere may be some impact on these figures from what\\'s called the Rotterdam Effect, but it makes a relatively small difference, as we discussed in this Reality Check.\\nBut of course the EU does not negotiate only as a bloc - it is made up of 27 other countries, some of which will be less concerned about UK trade than others and all of which will get to vote on the eventual deal.'], 'summary': ['A light-hearted appeal to trace a shoplifter by police in Lanarkshire has become a social media hit.', 'International trade secretary Liam Fox gave a speech on Thursday about the benefits of free trade.'], 'id': ['40898320', '37510253']}\n"
     ]
    }
   ],
   "source": [
    "show_random_elements(raw_datsets[\"train\"], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"rouge\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}, usage: \"\"\"\n",
       "Calculates average rouge scores for a list of hypotheses and references\n",
       "Args:\n",
       "    predictions: list of predictions to score. Each prediction\n",
       "        should be a string with tokens separated by spaces.\n",
       "    references: list of reference for each prediction. Each\n",
       "        reference should be a string with tokens separated by spaces.\n",
       "    rouge_types: A list of rouge types to calculate.\n",
       "        Valid names:\n",
       "        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n",
       "        `\"rougeL\"`: Longest common subsequence based scoring.\n",
       "        `\"rougeLSum\"`: rougeLsum splits text using `\"\n",
       "\"`.\n",
       "        See details in https://github.com/huggingface/datasets/issues/617\n",
       "    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n",
       "    use_aggregator: Return aggregates if this is set to True\n",
       "Returns:\n",
       "    rouge1: rouge_1 (precision, recall, f1),\n",
       "    rouge2: rouge_2 (precision, recall, f1),\n",
       "    rougeL: rouge_l (precision, recall, f1),\n",
       "    rougeLsum: rouge_lsum (precision, recall, f1)\n",
       "Examples:\n",
       "\n",
       "    >>> rouge = datasets.load_metric('rouge')\n",
       "    >>> predictions = [\"hello there\", \"general kenobi\"]\n",
       "    >>> references = [\"hello there\", \"general kenobi\"]\n",
       "    >>> results = rouge.compute(predictions=predictions, references=references)\n",
       "    >>> print(list(results.keys()))\n",
       "    ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
       "    >>> print(results[\"rouge1\"])\n",
       "    AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))\n",
       "    >>> print(results[\"rouge1\"].mid.fmeasure)\n",
       "    1.0\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_preds = [\"hello there\", \"general kenobi\"]\n",
    "fake_labels = [\"hello there\", \"general kenobi\"]\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the transformers `Tokenizer` to tokenize the imputs, this converts the tokens to the IDs in the pretrained vocabularly, formats the inputs for the model, and generate other inputs that the model needs. \n",
    "\n",
    "By instantiating the Â´AutoTokenizer.from_pretrainedÂ´ method, we get a tokenizer specific to the model architecture & the vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /Users/JENSAM/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /Users/JENSAM/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /Users/JENSAM/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /Users/JENSAM/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /Users/JENSAM/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 8667, 1139, 1271, 1110, 11265, 2921, 117, 146, 1686, 1105, 1710, 1120, 1275, 5245, 1158, 1715, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello my name is Boris Johnson, I live and party at 10 Downing Street.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 8667, 1139, 1271, 1110, 11265, 2921, 117, 146, 1686, 1105, 1710, 1120, 1275, 5245, 1158, 1715, 119, 102, 1188, 1110, 170, 175, 20356, 5650, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello my name is Boris Johnson, I live and party at 10 Downing Street.\", \"This is a fabulous sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prefix the inputs with \"summarize\" when using the T5 model checkpoint as it can also do translation and it needs to know which task to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"summarize\"\n",
    "else:\n",
    "    prefix = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to preprocess the samples, give them to the Â´tokenizerÂ´ using the argument Â´truncation=trueÂ´. This will truncate input that are longer than what the model can handle will be truncated to the maximum length accpeted by the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1109, 1554, 2616, 1104, 3290, 1107, 8102, 5272, 117, 1141, 1104, 1103, 1877, 4997, 4634, 117, 1110, 1253, 1217, 14758, 119, 20777, 8341, 1250, 1110, 7173, 1107, 11679, 6196, 1105, 1242, 4744, 1107, 153, 3051, 8350, 6662, 3118, 6118, 4634, 1118, 2288, 1447, 119, 20223, 1113, 1103, 1745, 3153, 25229, 1339, 23730, 1496, 1106, 3290, 1120, 1103, 21372, 5541, 15709, 13890, 119, 2408, 5028, 1105, 7036, 1116, 1127, 4634, 1118, 9420, 1107, 8102, 5272, 1170, 1103, 1595, 140, 8871, 1166, 12712, 1174, 1154, 1103, 1411, 119, 1752, 2110, 18634, 1457, 27793, 1320, 3891, 1103, 1298, 1106, 25151, 1103, 3290, 119, 1109, 5635, 13275, 1174, 170, 13223, 2095, 117, 9420, 1242, 2595, 4625, 1113, 3006, 1715, 118, 1103, 1514, 6001, 17213, 14154, 119, 2893, 6347, 9727, 117, 1150, 8300, 1103, 140, 23339, 18375, 1134, 1108, 6118, 4634, 117, 1163, 1131, 1180, 1136, 6088, 1103, 4321, 118, 4792, 2593, 1517, 1103, 7870, 1855, 119, 1438, 117, 1131, 1163, 1167, 3843, 5838, 1250, 1180, 1138, 1151, 2446, 1149, 1106, 4989, 1103, 13223, 2095, 1225, 1136, 8693, 119, 107, 1135, 1110, 2846, 1133, 146, 1202, 1341, 1175, 1110, 1177, 1277, 12623, 1111, 12786, 1306, 2087, 3377, 1105, 1103, 27453, 1582, 118, 1105, 146, 5733, 8856, 1115, 118, 1133, 1122, 1110, 1593, 1176, 1195, 112, 1231, 16778, 1137, 6278, 117, 107, 1131, 1163, 119, 107, 1337, 1336, 1136, 1129, 2276, 1133, 1122, 1110, 3229, 1139, 7281, 1166, 1103, 1314, 1374, 1552, 119, 107, 2009, 1127, 1128, 1136, 2407, 1106, 1494, 1366, 170, 2113, 1167, 1165, 1103, 5183, 1105, 1103, 8819, 10427, 1116, 1125, 2065, 1149, 136, 107, 5459, 117, 170, 7870, 10427, 2606, 1107, 1282, 1506, 1103, 25746, 1272, 1104, 1103, 4836, 4458, 119, 153, 3051, 8350, 1108, 6118, 1855, 1118, 2645, 117, 14798, 1158, 3675, 1106, 8698, 1167, 18422, 1107, 1103, 1298, 119, 3250, 25746, 1761, 1144, 1508, 170, 2190, 1113, 1157, 3265, 1104, 1103, 4744, 4997, 4634, 1105, 7016, 1138, 1151, 9497, 1136, 1106, 8429, 8354, 5300, 119, 1109, 4560, 1786, 112, 188, 5874, 3250, 2301, 3230, 14706, 1926, 1108, 1107, 11679, 6196, 1113, 6356, 1106, 1267, 1103, 2820, 1148, 1289, 119, 1124, 1163, 1122, 1108, 1696, 1106, 1243, 1103, 7870, 3636, 2197, 1268, 1133, 5534, 3675, 1106, 2420, 1146, 1103, 1965, 119, 107, 146, 1108, 2385, 1678, 170, 4197, 1118, 1103, 2971, 1104, 3290, 1115, 1144, 1151, 1694, 117, 107, 1119, 1163, 119, 107, 15580, 1122, 1110, 1762, 118, 4440, 1111, 1234, 1150, 1138, 1151, 2257, 1149, 1104, 1147, 4481, 1105, 1103, 3772, 1113, 5028, 119, 107, 1124, 1163, 1122, 1108, 1696, 1115, 107, 5670, 3343, 107, 1127, 1678, 1106, 3244, 1103, 1877, 1211, 8018, 1105, 170, 2330, 27646, 1508, 1107, 1282, 1111, 7870, 13347, 2714, 119, 4373, 1128, 1151, 4634, 1118, 9420, 1107, 12786, 1306, 2087, 3377, 1105, 24833, 1137, 1103, 25746, 136, 4630, 1366, 1164, 1240, 2541, 1104, 1103, 2820, 1105, 1293, 1122, 1108, 8630, 119, 18653, 11922, 1366, 1113, 14516, 10493, 15725, 119, 2371, 137, 171, 1830, 1665, 119, 1884, 119, 26006, 1137, 3840, 1306, 2087, 3377, 137, 171, 1830, 1665, 119, 1884, 119, 26006, 119, 102], [101, 138, 1783, 8819, 1355, 1228, 1120, 1103, 10450, 9859, 1107, 5977, 1715, 1120, 1164, 5129, 131, 1406, 21948, 1942, 1113, 4306, 1105, 6065, 1127, 1455, 1106, 1817, 1103, 3415, 119, 1249, 1152, 5260, 1796, 1152, 1486, 1103, 1160, 8049, 117, 8806, 1334, 118, 1118, 118, 1334, 1107, 1103, 1610, 2493, 117, 28109, 1118, 8150, 119, 1448, 1104, 1103, 2465, 2114, 1110, 1121, 1860, 117, 1103, 1168, 1121, 1975, 1105, 6036, 119, 1135, 1108, 1147, 1148, 1480, 1107, 2579, 2270, 119, 1109, 3445, 1104, 1141, 1104, 1103, 8049, 1163, 1242, 1104, 1103, 4861, 1125, 1286, 2357, 21924, 1113, 2313, 1105, 1292, 1125, 1151, 3072, 119, 2695, 2114, 1138, 6967, 5627, 8648, 1105, 1209, 3295, 1147, 2465, 1104, 1103, 1564, 3153, 1224, 1190, 1152, 1125, 2919, 119, 3284, 1138, 12181, 1111, 1869, 1164, 1103, 2035, 119, 1130, 20080, 1681, 9096, 1163, 131, 107, 1135, 2691, 1112, 1463, 1103, 1783, 1408, 1223, 1141, 1104, 1103, 8049, 1196, 9243, 1106, 1103, 1248, 119, 107, 1799, 1103, 6129, 2612, 1110, 1253, 1223, 4449, 117, 1122, 1110, 1354, 1115, 1103, 1783, 1108, 1408, 9938, 119, 107, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[101, 17508, 118, 1146, 2500, 1132, 5542, 1506, 1103, 3250, 25746, 1105, 12786, 1306, 2087, 3377, 1105, 24833, 1170, 9420, 2416, 1118, 7007, 2748, 119, 102], [101, 1960, 7798, 8049, 1138, 1151, 3072, 1118, 1783, 1107, 170, 6321, 170, 15379, 2035, 1107, 10189, 1331, 2642, 119, 102]]}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_function(raw_datsets['train'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/JENSAM/GIT/edc22-nlp'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a96baf811eb6eea8d779987ba815633c17fd2b4081a8b3336f6882fcaead153"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
